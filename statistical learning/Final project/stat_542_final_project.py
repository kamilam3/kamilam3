# -*- coding: utf-8 -*-
"""Stat 542 Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sDQtP76VwNEnTmGEpKV5nOOwv7vyWK4Y

## Initialize Fashion MNIST Datasets
"""

!pip install kneed

import io
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from kneed import KneeLocator

#Import dataset files
from google.colab import files
uploaded = files.upload()

#Load datasets
train = pd.read_csv("fashion-mnist_train.csv")
test = pd.read_csv("fashion-mnist_test.csv")

#View first 10 rows of train data
train.head(10)

#View first 10 rows of test data
test.head(10)

"""## Summary Statistics, Data Processing, and Unsupervised Learning

### Frequency Table

#### Frequency Table for $Y_{train}$
"""

train_freq=pd.crosstab(train['label'], 'frequency')

train_freq

"""#### Frequency Table for $Y_{test}$"""

test_freq=pd.crosstab(index=test['label'], columns='frequency')

test_freq

"""###Min, Max, Median, and Mode of each X column in the training data"""

X = train.iloc[:, 1:]

max = pd.DataFrame(X.max(0))
min = pd.DataFrame(X.min(0))
median = X.median(0).transpose()
mode = X.mode(0).transpose()


descriptive = pd.concat([max, min, median, mode], axis=1, ignore_index=True)
descriptive.rename({0: 'Max', 1: 'Min', 2: 'Median', 3: 'Mode'}, axis=1, inplace = True)
descriptive

X_test = test.iloc[:, 1:]

max2 = pd.DataFrame(X_test.max(0))
min2 = pd.DataFrame(X_test.min(0))
median2 = X_test.median(0).transpose()
mode2 = X_test.mode(0).transpose()

descriptive2 = pd.concat([max2, min2, median2, mode2], axis=1, ignore_index=True)
descriptive2.rename({0: 'Max', 1: 'Min', 2: 'Median', 3: 'Mode'}, axis=1, inplace = True)
descriptive2

"""## Standardize Data"""

train_scale =train.iloc[:,1:]/255
train_scale.head()

test_scale =test.iloc[:,1:]/255
test_scale.head()

max3 = pd.DataFrame(train_scale.max(0))
min3 = pd.DataFrame(train_scale.min(0))
median3 = train_scale.median(0).transpose()
mode3 = train_scale.mode(0).transpose()

descriptive3 = pd.concat([max3, min3, median3, mode3], axis=1, ignore_index=True)
descriptive3.rename({0: 'Max', 1: 'Min', 2: 'Median', 3: 'Mode'}, axis=1, inplace = True)
descriptive3

max4 = pd.DataFrame(test_scale.max(0))
min4 = pd.DataFrame(test_scale.min(0))
median4 = test_scale.median(0).transpose()
mode4 = test_scale.mode(0).transpose()

descriptive4 = pd.concat([max4, min4, median4, mode4], axis=1, ignore_index=True)
descriptive4.rename({0: 'Max', 1: 'Min', 2: 'Median', 3: 'Mode'}, axis=1, inplace = True)
descriptive4

def min_max(dataset):
  dataset_min_max= dataset.copy()
  for column in dataset_min_max.columns:
    dataset_min_max[column] = (dataset_min_max[column] - dataset_min_max[column].min()) / (dataset_min_max[column].max() - dataset_min_max[column].min())
  return dataset_min_max

train_min_max=min_max(train.iloc[:,1:])
test_min_max=min_max(test.iloc[:,1:])

test_min_max.head()

train_min_max.head()

"""### Clustering Algorithm #1: Kmeans Clustering"""

#Choose optimal number of clusters for Kmeans clustering
kmeans_args = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,
    "random_state": 542,
    }

#Get SSE values for each k
sse = []
for k in range(1, 11):
  kmeans = KMeans(n_clusters = k, **kmeans_args)
  kmeans.fit(train_scale)
  sse.append(kmeans.inertia_)

#Elbow plot
plt.style.use("fivethirtyeight")
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

#Optimal k
kl = KneeLocator(range(1, 11), sse, curve = "convex", direction = "decreasing")
kl.elbow

#Perform Kmeans clustering with optimal k
kmeans = KMeans(
    init = "random",
    n_clusters = kl.elbow,
    n_init = 10,
    max_iter = 300,
    random_state = 542
    )

kmeans.fit(train_scale)

kmeans.labels_[:]

kclus_data = pd.DataFrame({'label': np.array(train.iloc[:,0]), 'cluster': kmeans.labels_[:]}, columns = ['label', 'cluster'])
kclus_data

#Get dominating label in each cluster
print(kclus_data.groupby('cluster').agg(lambda x:x.value_counts().index[0]))

"""Clustering Algorithm #2: Self-organized Map"""

!pip install minisom

"""For the number of neurons, there is no set recommendation. Some suggestions I've found include:
1. 5*sqrt(n obs), which would give about 1225 (seems too many)
2. About 10 neurons per expected class, which would give 100 (10x10 seems like a lot, but more reasonable)
3. Resulting in no more than 5% of clusters to be empty
4. A "big" map would be 4x the default number 4 x 5 x sqrt(n obs) = 4,900 or a "small" map would be 0.25x the default 0.25 x 5 x sqrt(n obs) = 306

Overall no consensus, but 10x10 seems like the smallest recommendation to start.
"""

from minisom import MiniSom
train_scale = np.array(train_scale)
som = MiniSom(10, 10, 784, sigma=0.5, learning_rate=0.5)
som.random_weights_init(train_scale)
som.train_random(train_scale, 100)

from matplotlib.pyplot import plot,axis,show,pcolor,colorbar,bone

bone()
pcolor(som.distance_map().T) # plotting the distance map as background
colorbar()

t = train["label"]
w = np.zeros(len(t), dtype=int)
clusters = pd.DataFrame(np.array(range(100)).reshape(10,10))

for cnt, xx in enumerate(train_scale):
  w[cnt] = clusters.iloc[som.winner(xx)[0], som.winner(xx)[1]]

unique, counts = np.unique(np.array(w), return_counts=True)

np.array((unique, counts)).T

som_data = pd.DataFrame({'label': np.array(train.iloc[:,0]), 'cluster': w}, columns = ['label', 'cluster'])
som_data

#Get dominating label in each cluster
som_data.groupby('cluster').agg(lambda x:x.value_counts().index[0])

"""## Multi-class Classification Model

### Random Forest

#### Tuning Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import numpy as np

Y_train=train['label']
Y_test=test['label']

tree_number=np.arange(100,450,50)
accuracy=np.zeros(len(tree_number))
i=0

#Find n_estimators-parameter in [100,150,200, ..., 400] that will maximize accuracy
for tree in tree_number:
    clf=RandomForestClassifier(n_estimators=tree, criterion= 'entropy', max_depth=100, random_state = 542)
    clf.fit(train_min_max, Y_train)
    Y_pred_rf=clf.predict(test_min_max)
    accuracy_rf=np.mean(Y_pred_rf==Y_test)
    accuracy[i]=accuracy_rf
    i=i+1

max_index2=np.where(accuracy == np.amax(accuracy))
print('maximizing accuracy number of tree = ', tree_number[max_index2[0][0]])

import matplotlib.pyplot as plt

#Plot the points
plt.plot(tree_number, accuracy)

#Add axes labels
plt.xlabel('number of trees in Random forest')
plt.ylabel('the corresponding accuracy')

#Add title
plt.title('Number of trees and the corresponding accuracy for Random Forest')

plt.show()

"""#### Final Ranfom Forest Model"""

#Create random forest model
clf=RandomForestClassifier(n_estimators=tree_number[max_index2[0][0]], criterion= 'entropy', max_depth=100, random_state = 542)

#Fit random forest model on training data
clf.fit(train_scale, Y_train)

#Make prediction for testing data
Y_pred_RF=clf.predict(test_scale)

#Calculate accuracy
accuracy_rf=metrics.accuracy_score(Y_test, Y_pred_RF)

#Summarize random forest predictions
print("Accuracy for Random Forest: %.2f%%" % (accuracy_rf * 100.0))
print("Misclassification rate for Random Forest: %.2f%%" % ((1-accuracy_rf) * 100.0))

"""#### Confusion matrix for Random Forest"""

from sklearn.metrics import confusion_matrix

conf_matrix_rf=pd.DataFrame(confusion_matrix(Y_test, Y_pred_RF))
conf_matrix_rf.columns =['predicted class 0', ' predicted class 1', ' predicted class 2', ' predicted class 3', ' predicted class 4', 'predicted class 5',
                         'predicted class 6','predicted class 7', 'predicted class 8', 'predicted class 9']


conf_matrix_rf

"""### LDA"""

from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.datasets import make_classification
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

#Create LDA model
LDA_model = LDA()

#Fit LDA model on training data
LDA_model.fit(train_scale, Y_train)

#Make predictions for testing data
Y_pred_lda = LDA_model.predict(test_scale)

#Calculate accuracy
accuracy_lda=np.mean(Y_pred_lda==Y_test)

#Summarize LDA predictions
print('Accuracy for LDA: %.2f%%' % (accuracy_lda * 100.0))
print('Misclassification rate for LDA: %.2f%%' % ((1-accuracy_lda) * 100.0))

"""#### Confusion matrix for LDA"""

from sklearn.metrics import confusion_matrix

conf_matrix_lda=pd.DataFrame(confusion_matrix(Y_test, Y_pred_lda))
conf_matrix_lda.columns =['predicted class 0', ' predicted class 1', ' predicted class 2', ' predicted class 3', ' predicted class 4', 'predicted class 5',
                         'predicted class 6','predicted class 7', 'predicted class 8', 'predicted class 9']


conf_matrix_lda

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

#Create logistic regression model for multi-class data
model_lr = LogisticRegression(multi_class='multinomial', max_iter=1000, solver = 'sag', random_state = 542)

#Fit logistic regression model on training data
model_lr.fit(train_scale,Y_train)

#Make predictions for testing data
Y_pred_lr=model_lr.predict(test_scale)

#Calculate accuracy
accuracy_lr=np.mean(Y_pred_lr==Y_test)


#Summarize logistic regression predictions
print('Accuracy for Logistic Regression : %.2f%%' % (accuracy_lr * 100.0))
print('Misclassification rate for Logistic Regression : %.2f%%' % ((1-accuracy_lr) * 100.0))

"""#### Confusion matrix for Logistic Regression"""

from sklearn.metrics import confusion_matrix

conf_matrix_lr=pd.DataFrame(confusion_matrix(Y_test, Y_pred_lr))
conf_matrix_lr.columns =['predicted class 0', ' predicted class 1', ' predicted class 2', ' predicted class 3', ' predicted class 4', 'predicted class 5',
                         'predicted class 6','predicted class 7', 'predicted class 8', 'predicted class 9']


conf_matrix_lr

"""#### Summary for multi-classification methods (Random Forest, LDA, Logistic Regression)"""

summary = {'Type': ['Random Forest', 'LDA', 'Logistic Regression'], 'Accuracy rate': [accuracy_rf,accuracy_lda,accuracy_lr],
           'classification error':[1-accuracy_rf,1-accuracy_lda,1-accuracy_lr]}

summary_table = pd.DataFrame(summary)
summary_table

"""## Part 3 : Random Forest (from previous section), Gradient Boosting, and KNN

### Gradient Boosting
"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

#Create GBM model
gbm_model = XGBClassifier(objective = 'multi:softmax', random_state = 542)

#Fit the model on training data
gbm_model.fit(train_scale, Y_train)

#Make predictions for test data
y_pred_gbm = gbm_model.predict(test_scale.values)
preds_gbm = [round(value) for value in y_pred_gbm]

#Summarize GBM predictions
gbm_accuracy = accuracy_score(Y_test, preds_gbm)
print("Accuracy for GBM: %.2f%%" % (gbm_accuracy * 100.0))
print('Misclassification rate for GBM: %.2f%%' % ((1-gbm_accuracy) * 100.0))

"""#### Tune GBM"""

#Fit final tuned GBM on training data
gbm_model_tuned = XGBClassifier(n_estimators = 100, loss = 'deviance', max_depth = 10, objective = 'multi:softmax', random_state = 542)
gbm_model_tuned.fit(train_scale, Y_train)

#Make predictions for testing data
y_pred_gbm_tuned = gbm_model_tuned.predict(test_scale.values)
preds_gbm_tuned = [round(value) for value in y_pred_gbm_tuned]

#Summarize tuned GBM predictions
gbm_tuned_accuracy = np.mean(preds_gbm_tuned == Y_test)
print("Accuracy GBM tuned: %.2f%%" % (gbm_tuned_accuracy * 100.0))
print('Misclassification rate for GBM tuned: %.2f%%' % ((1-gbm_tuned_accuracy) * 100.0))

"""### KNN

"""

from sklearn.neighbors import KNeighborsClassifier

accuracy=np.zeros(11)
# find k-parameter 1-10 that will maximize accuracy
for i in range(1,11):
    knn_classifier = KNeighborsClassifier(n_neighbors=i)
    knn_classifier.fit(train_scale, Y_train)
    Y_pred_knn = knn_classifier.predict(test_scale)
    accuracy_knn=np.mean(Y_pred_knn==Y_test)
    accuracy[i]=accuracy_knn

max_index=np.where(accuracy == np.amax(accuracy))

print('maximizing accuracy k = ', max_index[0][0])

accuracy

#Create KNN model using k = max_index
knn_classifier = KNeighborsClassifier(n_neighbors=max_index[0][0])

#Fit KNN model on training data
knn_classifier.fit(train_scale, Y_train)

#Make predictions for testing data
Y_pred_knn = knn_classifier.predict(test_scale)

#Calculate accuracy
accuracy_knn=np.mean(Y_pred_knn==Y_test)

#Summarize KNN predictions
print('Accuracy : %f' % accuracy_knn)

"""### Ensemble method: Weighted averaging"""

from sklearn.ensemble import VotingClassifier

#List of 3 models
models = list()
models.append(('GB', XGBClassifier(n_estimators = 100, loss = 'deviance', max_depth = 10, objective = 'multi:softmax', random_state = 542)))
models.append(('KNN', KNeighborsClassifier(n_neighbors=max_index[0][0])))
models.append(('Random Forest', RandomForestClassifier(n_estimators=350, criterion= 'entropy', max_depth=100, random_state = 542)))

#Use accuracy of models to create weights
accuracy_models=[gbm_tuned_accuracy, accuracy_knn, accuracy_rf]

ensemble = VotingClassifier(estimators=models, voting='soft', weights=accuracy_models)

#Fit the ensemble on the training dataset
ensemble.fit(train_scale, Y_train)

#Make predictions on testing set
yhat = ensemble.predict(test_scale.values)

#Calculate accuracy
accuracy_weighted_aver = accuracy_score(Y_test, yhat)

print('Weighted Averaging Accuracy: %.3f' % (accuracy_weighted_aver))

#Summarize predictions
summary2 = {'Type': ['Random Forest', 'GBM', 'KNN', "Ensemble weighted average"], 'Accuracy rate': [accuracy_rf, gbm_tuned_accuracy, accuracy_knn, accuracy_weighted_aver],
           'classification error':[1-accuracy_rf,1-gbm_tuned_accuracy,1-accuracy_knn, 1-accuracy_weighted_aver]}

summary_table2 = pd.DataFrame(summary2)
summary_table2